{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "all = pd.concat([train, test], axis = 0)\n",
    "transactions = pd.read_csv(\"data/transactions.csv\")\n",
    "holiday_events = pd.read_csv(\"data/holidays_events.csv\")\n",
    "stores = pd.read_csv(\"data/stores.csv\")\n",
    "oil = pd.read_csv(\"data/oil.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3734286/3777914067.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  holiday_events = holiday_events.append(tr).reset_index(drop = True)\n",
      "/tmp/ipykernel_3734286/3777914067.py:16: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  holiday_events[\"description\"] = holiday_events[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n",
      "/tmp/ipykernel_3734286/3777914067.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  holiday_events[\"description\"] = holiday_events[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n",
      "/tmp/ipykernel_3734286/3777914067.py:57: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  date_info[\"week\"] = date_info[\"date\"].dt.week\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/ekrembayar/holiday_events-events-data-manipulation-time-features/notebook\n",
    "# Deal multiple holiday\n",
    "# 확인 후 리팩토링 필요\n",
    "\n",
    "tr1 = holiday_events[(holiday_events.type == \"Holiday\") & (holiday_events.transferred == True)].drop(\"transferred\", axis = 1).reset_index(drop = True)\n",
    "tr2 = holiday_events[(holiday_events.type == \"Transfer\")].drop(\"transferred\", axis = 1).reset_index(drop = True)\n",
    "tr = pd.concat([tr1,tr2], axis = 1)\n",
    "tr = tr.iloc[:, [5,1,2,3,4]]\n",
    "\n",
    "holiday_events = holiday_events[(holiday_events.transferred == False) & (holiday_events.type != \"Transfer\")].drop(\"transferred\", axis = 1)\n",
    "holiday_events = holiday_events.append(tr).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# Additional Holidays\n",
    "# ------------------------------------------------------\n",
    "holiday_events[\"description\"] = holiday_events[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n",
    "holiday_events[\"type\"] = np.where(holiday_events[\"type\"] == \"Additional\", \"Holiday\", holiday_events[\"type\"])\n",
    "\n",
    "# Bridge Holidays\n",
    "# ------------------------------------------------------\n",
    "holiday_events[\"description\"] = holiday_events[\"description\"].str.replace(\"Puente \", \"\")\n",
    "holiday_events[\"type\"] = np.where(holiday_events[\"type\"] == \"Bridge\", \"Holiday\", holiday_events[\"type\"])\n",
    "\n",
    " \n",
    "# Work Day Holidays, that is meant to payback the Bridge.\n",
    "# ------------------------------------------------------\n",
    "work_day = holiday_events[holiday_events.type == \"Work Day\"]  \n",
    "holiday_events = holiday_events[holiday_events.type != \"Work Day\"]  \n",
    "\n",
    "\n",
    "# Split\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Events are national\n",
    "events = holiday_events[holiday_events.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis = 1).rename({\"description\":\"events\"}, axis = 1)\n",
    "\n",
    "holiday_events = holiday_events[holiday_events.type != \"Event\"].drop(\"type\", axis = 1)\n",
    "regional = holiday_events[holiday_events.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n",
    "national = holiday_events[holiday_events.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis = 1).drop([\"locale\", \"locale_name\"], axis = 1).drop_duplicates()\n",
    "local = holiday_events[holiday_events.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n",
    "\n",
    "# EVENTS\n",
    "events[\"events\"] =np.where(events.events.str.contains(\"futbol\"), \"Futbol\", events.events)\n",
    "\n",
    "# Merge\n",
    "holiday_events = pd.concat([events, regional, national, local], axis = 0).reset_index(drop = True)\n",
    "\n",
    "# convert datetime to str\n",
    "# some additional date features\n",
    "date_info = pd.DataFrame({\"date\": pd.date_range(start=all[\"date\"].min(), end=all[\"date\"].max())})\n",
    "\n",
    "date_info[\"year\"] = date_info[\"date\"].dt.year\n",
    "date_info[\"month\"] = date_info[\"date\"].dt.month\n",
    "date_info[\"day\"] = date_info[\"date\"].dt.day\n",
    "date_info[\"dayofweek\"] = date_info[\"date\"].dt.dayofweek\n",
    "date_info[\"weekend\"] = (date_info[\"dayofweek\"] >= 5).astype(int)\n",
    "date_info[\"week\"] = date_info[\"date\"].dt.week\n",
    "date_info[\"quarter\"] = date_info[\"date\"].dt.quarter\n",
    "date_info[\"season\"] = date_info[\"month\"] % 12 // 3 + 1\n",
    "\n",
    "date_info[\"date\"] = date_info[\"date\"].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28507</th>\n",
       "      <td>3029395</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28508</th>\n",
       "      <td>3029396</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28509</th>\n",
       "      <td>3029397</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28510</th>\n",
       "      <td>3029398</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28511</th>\n",
       "      <td>3029399</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3029400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id        date  store_nbr                      family  sales  \\\n",
       "0            0  2013-01-01          1                  AUTOMOTIVE    0.0   \n",
       "1            1  2013-01-01          1                   BABY CARE    0.0   \n",
       "2            2  2013-01-01          1                      BEAUTY    0.0   \n",
       "3            3  2013-01-01          1                   BEVERAGES    0.0   \n",
       "4            4  2013-01-01          1                       BOOKS    0.0   \n",
       "...        ...         ...        ...                         ...    ...   \n",
       "28507  3029395  2017-08-31          9                     POULTRY    NaN   \n",
       "28508  3029396  2017-08-31          9              PREPARED FOODS    NaN   \n",
       "28509  3029397  2017-08-31          9                     PRODUCE    NaN   \n",
       "28510  3029398  2017-08-31          9  SCHOOL AND OFFICE SUPPLIES    NaN   \n",
       "28511  3029399  2017-08-31          9                     SEAFOOD    NaN   \n",
       "\n",
       "       onpromotion  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "28507            1  \n",
       "28508            0  \n",
       "28509            1  \n",
       "28510            9  \n",
       "28511            0  \n",
       "\n",
       "[3029400 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train\n",
    "merged = pd.merge(all, date_info, on=[\"date\"], how=\"outer\")\n",
    "merged = pd.merge(merged, transactions, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "merged = pd.merge(merged, stores, on=[\"store_nbr\"], how=\"left\")\n",
    "merged = pd.merge(merged, holiday_events, on=[\"date\", \"city\", \"state\"], how=\"left\")\n",
    "merged = pd.merge(merged, oil, on=[\"date\"], how=\"left\")\n",
    "\n",
    "# 없는 값은 거래가 발생하지 않은 것으로 가정\n",
    "merged[\"transactions\"] = merged[\"transactions\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na\n",
    "# TODO: Normal은 0으로 취급될 수 있게 변경 필요\n",
    "merged[\"events\"] = merged[\"events\"].fillna(\"Normal\")\n",
    "merged[\"holiday_regional\"] = merged[\"holiday_regional\"].fillna(\"Normal\")\n",
    "merged[\"holiday_national\"] = merged[\"holiday_national\"].fillna(\"Normal\")\n",
    "merged[\"holiday_local\"] = merged[\"holiday_local\"].fillna(\"Normal\")\n",
    "merged[\"id\"] = merged[\"id\"].fillna(-1)\n",
    "merged = merged.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "date                0\n",
       "store_nbr           0\n",
       "family              0\n",
       "sales               0\n",
       "onpromotion         0\n",
       "year                0\n",
       "month               0\n",
       "day                 0\n",
       "dayofweek           0\n",
       "weekend             0\n",
       "week                0\n",
       "quarter             0\n",
       "season              0\n",
       "transactions        0\n",
       "city                0\n",
       "state               0\n",
       "type                0\n",
       "cluster             0\n",
       "events              0\n",
       "holiday_regional    0\n",
       "holiday_national    0\n",
       "holiday_local       0\n",
       "dcoilwtico          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isna(merged).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>transactions</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>events</th>\n",
       "      <th>holiday_regional</th>\n",
       "      <th>holiday_national</th>\n",
       "      <th>holiday_local</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029399</th>\n",
       "      <td>3029399.0</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9.0</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>B</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>47.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029400</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Navidad</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029401</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2014-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Navidad</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029402</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2015-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Navidad</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029403</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Navidad</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3029404 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id        date  store_nbr      family  sales  onpromotion  \\\n",
       "0              0.0  2013-01-01        1.0  AUTOMOTIVE    0.0          0.0   \n",
       "1              1.0  2013-01-01        1.0   BABY CARE    0.0          0.0   \n",
       "2              2.0  2013-01-01        1.0      BEAUTY    0.0          0.0   \n",
       "3              3.0  2013-01-01        1.0   BEVERAGES    0.0          0.0   \n",
       "4              4.0  2013-01-01        1.0       BOOKS    0.0          0.0   \n",
       "...            ...         ...        ...         ...    ...          ...   \n",
       "3029399  3029399.0  2017-08-31        9.0     SEAFOOD    0.0          0.0   \n",
       "3029400       -1.0  2013-12-25        0.0           0    0.0          0.0   \n",
       "3029401       -1.0  2014-12-25        0.0           0    0.0          0.0   \n",
       "3029402       -1.0  2015-12-25        0.0           0    0.0          0.0   \n",
       "3029403       -1.0  2016-12-25        0.0           0    0.0          0.0   \n",
       "\n",
       "         year  month  day  dayofweek  ...  transactions   city      state  \\\n",
       "0        2013      1    1          1  ...           0.0  Quito  Pichincha   \n",
       "1        2013      1    1          1  ...           0.0  Quito  Pichincha   \n",
       "2        2013      1    1          1  ...           0.0  Quito  Pichincha   \n",
       "3        2013      1    1          1  ...           0.0  Quito  Pichincha   \n",
       "4        2013      1    1          1  ...           0.0  Quito  Pichincha   \n",
       "...       ...    ...  ...        ...  ...           ...    ...        ...   \n",
       "3029399  2017      8   31          3  ...           0.0  Quito  Pichincha   \n",
       "3029400  2013     12   25          2  ...           0.0      0          0   \n",
       "3029401  2014     12   25          3  ...           0.0      0          0   \n",
       "3029402  2015     12   25          4  ...           0.0      0          0   \n",
       "3029403  2016     12   25          6  ...           0.0      0          0   \n",
       "\n",
       "         type  cluster  events holiday_regional holiday_national  \\\n",
       "0           D     13.0  Normal           Normal           Normal   \n",
       "1           D     13.0  Normal           Normal           Normal   \n",
       "2           D     13.0  Normal           Normal           Normal   \n",
       "3           D     13.0  Normal           Normal           Normal   \n",
       "4           D     13.0  Normal           Normal           Normal   \n",
       "...       ...      ...     ...              ...              ...   \n",
       "3029399     B      6.0  Normal           Normal           Normal   \n",
       "3029400     0      0.0  Normal           Normal          Navidad   \n",
       "3029401     0      0.0  Normal           Normal          Navidad   \n",
       "3029402     0      0.0  Normal           Normal          Navidad   \n",
       "3029403     0      0.0  Normal           Normal          Navidad   \n",
       "\n",
       "         holiday_local dcoilwtico  \n",
       "0               Normal       0.00  \n",
       "1               Normal       0.00  \n",
       "2               Normal       0.00  \n",
       "3               Normal       0.00  \n",
       "4               Normal       0.00  \n",
       "...                ...        ...  \n",
       "3029399         Normal      47.26  \n",
       "3029400         Normal       0.00  \n",
       "3029401         Normal       0.00  \n",
       "3029402         Normal       0.00  \n",
       "3029403         Normal       0.00  \n",
       "\n",
       "[3029404 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: for simplicity\n",
    "# use only family=AUTOMOTIVE, store_nbr=1\n",
    "# idx = (merged[\"family\"] == \"AUTOMOTIVE\") & (merged[\"store_nbr\"] == 1)\n",
    "# merged = merged[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make item_id\n",
    "merged[\"item_id\"] = merged[\"family\"].astype(str) + \"_\" + merged[\"store_nbr\"].astype(str)\n",
    "\n",
    "# separate static and covariates\n",
    "drop_cols = []\n",
    "static_cols = [\"store_nbr\",\"family\",\"type\",\"cluster\", \"state\", \"city\"]\n",
    "\n",
    "static_features = merged[static_cols + [\"item_id\"]].drop_duplicates()\n",
    "static_features.set_index(\"item_id\", inplace=True)\n",
    "merged = merged.drop(columns=drop_cols + static_cols)\n",
    "\n",
    "merged[\"date\"] = pd.to_datetime(merged[\"date\"], format=\"%Y-%m-%d\")\n",
    "merged.sort_values(by=[\"item_id\",\"date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na\n",
    "# TODO: Normal은 0으로 취급될 수 있게 변경 필요\n",
    "merged[\"events\"] = merged[\"events\"].fillna(\"Normal\")\n",
    "merged[\"holiday_regional\"] = merged[\"holiday_regional\"].fillna(\"Normal\")\n",
    "merged[\"holiday_national\"] = merged[\"holiday_national\"].fillna(\"Normal\")\n",
    "merged[\"holiday_local\"] = merged[\"holiday_local\"].fillna(\"Normal\")\n",
    "merged[\"dcoilwtico\"] = merged[\"dcoilwtico\"].fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "categorical_col = [\"events\", \"holiday_regional\", \"holiday_national\", \"holiday_local\"]\n",
    "encoded = pd.get_dummies(merged[categorical_col + [\"item_id\", \"date\"]], columns=categorical_col)\n",
    "merged = pd.merge(merged, encoded, on=[\"item_id\", \"date\"], how=\"left\")\n",
    "merged.drop(columns=categorical_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>season</th>\n",
       "      <th>transactions</th>\n",
       "      <th>dcoilwtico</th>\n",
       "      <th>item_id</th>\n",
       "      <th>events_Normal</th>\n",
       "      <th>holiday_regional_Normal</th>\n",
       "      <th>holiday_national_Navidad</th>\n",
       "      <th>holiday_national_Normal</th>\n",
       "      <th>holiday_local_Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0_0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2014-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0_0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2015-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0_0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0_0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AUTOMOTIVE_1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       date  sales  onpromotion  year  month  day  dayofweek  weekend  \\\n",
       "0 -1.0 2013-12-25    0.0          0.0  2013     12   25          2        0   \n",
       "1 -1.0 2014-12-25    0.0          0.0  2014     12   25          3        0   \n",
       "2 -1.0 2015-12-25    0.0          0.0  2015     12   25          4        0   \n",
       "3 -1.0 2016-12-25    0.0          0.0  2016     12   25          6        1   \n",
       "4  0.0 2013-01-01    0.0          0.0  2013      1    1          1        0   \n",
       "\n",
       "   week  quarter  season  transactions  dcoilwtico         item_id  \\\n",
       "0    52        4       1           0.0         0.0           0_0.0   \n",
       "1    52        4       1           0.0         0.0           0_0.0   \n",
       "2    52        4       1           0.0         0.0           0_0.0   \n",
       "3    51        4       1           0.0         0.0           0_0.0   \n",
       "4     1        1       1           0.0         0.0  AUTOMOTIVE_1.0   \n",
       "\n",
       "   events_Normal  holiday_regional_Normal  holiday_national_Navidad  \\\n",
       "0              1                        1                         1   \n",
       "1              1                        1                         1   \n",
       "2              1                        1                         1   \n",
       "3              1                        1                         1   \n",
       "4              1                        1                         0   \n",
       "\n",
       "   holiday_national_Normal  holiday_local_Normal  \n",
       "0                        0                     1  \n",
       "1                        0                     1  \n",
       "2                        0                     1  \n",
       "3                        0                     1  \n",
       "4                        1                     1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TimeSeriesDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mysunk/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "timeseries_df = TimeSeriesDataFrame.from_data_frame(\n",
    "    merged,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"date\"\n",
    ")\n",
    "timeseries_df = timeseries_df.to_regular_index(freq=\"D\")\n",
    "\n",
    "categorical_col = [\"store_nbr\", \"family\", \"type\", \"cluster\", \"city\", \"state\"]\n",
    "for col in categorical_col:\n",
    "    # encoding to number\n",
    "    static_features[col] = static_features[col].astype(\"category\")\n",
    "    \n",
    "# for continuous timeseries\n",
    "timeseries_df = timeseries_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = timeseries_df.split_by_time(pd.Timestamp(\"2017-08-16\"))\n",
    "\n",
    "train_data.static_features = static_features\n",
    "test_data.static_features = static_features\n",
    "\n",
    "train_data[\"sales\"] = np.log1p(train_data[\"sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onpromotion',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dayofweek',\n",
       " 'weekend',\n",
       " 'week',\n",
       " 'quarter',\n",
       " 'season',\n",
       " 'transactions',\n",
       " 'dcoilwtico',\n",
       " 'events_Normal',\n",
       " 'holiday_regional_Normal',\n",
       " 'holiday_national_Navidad',\n",
       " 'holiday_national_Normal',\n",
       " 'holiday_local_Normal']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_covariates_cols = list(train_data.columns)\n",
    "known_covariates_cols.remove(\"sales\")\n",
    "known_covariates_cols.remove(\"id\")\n",
    "known_covariates_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model test\n",
    "* DeepAR\n",
    "\n",
    "context_length (int, optional) – lag값을 몇 개 참조할 것인지\n",
    "\n",
    "disable_static_features (bool, default = False)\n",
    "\n",
    "disable_known_covariates (bool, default = False)\n",
    "\n",
    "num_layers (int, default = 2) – Number of RNN layers\n",
    "\n",
    "hidden_size (int, default = 40) – Number of RNN cells for each layer\n",
    "\n",
    "dropout_rate (float, default = 0.1) – Dropout regularization parameter\n",
    "\n",
    "embedding_dimension (int, optional) – Dimension of the embeddings for categorical features (if None, defaults to [min(50, (cat+1)//2) for cat in cardinality])\n",
    "\n",
    "distr_output (gluonts.torch.distributions.DistributionOutput, default = StudentTOutput()) – Distribution to use to evaluate observations and sample predictions\n",
    "\n",
    "scaling (bool, default = True) – Whether to automatically scale the target values\n",
    "\n",
    "epochs (int, default = 100) – Number of epochs the model will be trained for\n",
    "\n",
    "batch_size (int, default = 64) – Size of batches used during training\n",
    "\n",
    "num_batches_per_epoch (int, default = 50) – Number of batches processed every epoch\n",
    "\n",
    "learning_rate (float, default = 1e-3,) – Learning rate used during training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이어서 재학습 가능한지? => 안되는듯..?\n",
    "* 먀지막에 저장되는 게 val_loss 기준 best인지 last epoch인지? => best인듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon.core as ag\n",
    "from autogluon.timeseries.splitter import MultiWindowSplitter\n",
    "splitter = MultiWindowSplitter(num_windows=1)\n",
    "predictor = TimeSeriesPredictor(known_covariates_names=known_covariates_cols, target=\"sales\", prediction_length=16, validation_splitter=splitter,\n",
    "                                eval_metric=\"RMSE\", verbosity = 4)\n",
    "# predictor.fit(train_data=train_data,\n",
    "#               hyperparameters={\n",
    "#             \"DeepAR\": {\n",
    "#                 \"hidden_size\": ag.space.Int(20, 100),\n",
    "#                 \"dropout_rate\": ag.space.Categorical(0.1, 0.3),\n",
    "#                 \"context_length\": ag.space.Categorical(16, 32, 64),\n",
    "#                 \"scaling\": ag.space.Categorical(True, False),\n",
    "#                 \"learning_rate\": ag.space.Real(1e-4, 1e-2, log=True),\n",
    "#                 \"batch_size\": ag.space.Categorical(16, 32, 64, 128),\n",
    "#             }\n",
    "#             },hyperparameter_tune_kwargs={\n",
    "#                 \"scheduler\": \"local\",\n",
    "#                 \"searcher\": \"auto\",\n",
    "#                 \"num_trials\": 30,\n",
    "#             }, enable_ensemble=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "================ TimeSeriesPredictor ================\n",
      "TimeSeriesPredictor.fit() called\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'evaluation_metric': 'RMSE',\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'hyperparameters': {'DeepAR': {'epochs': 10}},\n",
      " 'prediction_length': 16,\n",
      " 'random_seed': None,\n",
      " 'target': 'sales',\n",
      " 'time_limit': None}\n",
      "Provided training data set with 3009113 rows, 1783 items (item = single time series). Average time series length is 1687.7.\n",
      "Training artifacts will be saved to: /home/mysunk/PJT/automl_study/msun/AutogluonModels/ag-20230606_103643\n",
      "=====================================================\n",
      "Beginning AutoGluon training with TimeSeriesLearner \n",
      "AutoGluon will save models to AutogluonModels/ag-20230606_103643/\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being 'higher is better'. The reported score can be multiplied by -1 to get the metric value.\n",
      "\n",
      "Provided dataset contains following columns:\n",
      "\ttarget:           'sales'\n",
      "\tknown covariates: ['onpromotion', 'year', 'month', 'day', 'dayofweek', 'weekend', 'week', 'quarter', 'season', 'transactions', 'dcoilwtico', 'events_Normal', 'holiday_regional_Normal', 'holiday_national_Navidad', 'holiday_national_Normal', 'holiday_local_Normal']\n",
      "\tpast covariates:  ['id']\n",
      "Following types of static features have been inferred:\n",
      "\tcategorical:        ['store_nbr', 'family', 'type', 'cluster', 'state', 'city']\n",
      "\tcontinuous (float): []\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit \n",
      "tuning_data is None. Will use the last 1 windows (each with prediction_length = 16 time steps) as a hold-out validation set.\n",
      "Saving AutogluonModels/ag-20230606_103643/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20230606_103643/learner.pkl\n",
      "\n",
      "Starting training. Start time is 2023-06-06 19:36:48\n",
      "Saving AutogluonModels/ag-20230606_103643/utils/data/train.pkl\n",
      "Saving AutogluonModels/ag-20230606_103643/utils/data/val.pkl\n",
      "Models that will be trained: ['DeepAR']\n",
      "Training timeseries model DeepAR. \n",
      "\tFitting DeepAR with 'num_gpus': 0, 'num_cpus': 64\n",
      "GluonTS logging is turned on during training. Note that losses reported by GluonTS may not correspond to those specified via `eval_metric`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: AutogluonModels/ag-20230606_103643/models/DeepAR/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type        | Params | In sizes                                                           | Out sizes   \n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 43.5 K | [[1, 6], [1, 1], [1, 1125, 20], [1, 1125], [1, 1125], [1, 16, 20]] | [1, 100, 16]\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "43.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.5 K    Total params\n",
      "0.174     Total estimated model params size (MB)\n",
      "Epoch 0, global step 50: 'val_loss' reached 1.25662 (best 1.25662), saving model to 'AutogluonModels/ag-20230606_103643/models/DeepAR/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'val_loss' was not in top 1\n",
      "Epoch 2, global step 150: 'val_loss' reached 1.24652 (best 1.24652), saving model to 'AutogluonModels/ag-20230606_103643/models/DeepAR/lightning_logs/version_0/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
      "Epoch 3, global step 200: 'val_loss' was not in top 1\n",
      "Epoch 4, global step 250: 'val_loss' reached 1.24466 (best 1.24466), saving model to 'AutogluonModels/ag-20230606_103643/models/DeepAR/lightning_logs/version_0/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'val_loss' was not in top 1\n",
      "Epoch 6, global step 350: 'val_loss' was not in top 1\n",
      "Epoch 7, global step 400: 'val_loss' was not in top 1\n",
      "Epoch 8, global step 450: 'val_loss' was not in top 1\n",
      "Epoch 9, global step 500: 'val_loss' reached 1.24392 (best 1.24392), saving model to 'AutogluonModels/ag-20230606_103643/models/DeepAR/lightning_logs/version_0/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Removing lightning_logs directory AutogluonModels/ag-20230606_103643/models/DeepAR/lightning_logs\n",
      "Predicting with time series model DeepAR\n",
      "\tProvided data for prediction with 2980585 rows, 1783 items. Average time series length is 1671.668536174986.\n",
      "\t-0.9151       = Validation score (-RMSE)\n",
      "\t33.15   s     = Training runtime\n",
      "\t20.98   s     = Validation (prediction) runtime\n",
      "Saving AutogluonModels/ag-20230606_103643/models/DeepAR/model.pkl\n",
      "Not fitting ensemble as only 1 model was trained.\n",
      "Training complete. Models trained: ['DeepAR']\n",
      "Total runtime: 54.84 s\n",
      "Best model: DeepAR\n",
      "Best model score: -0.9151\n",
      "Saving AutogluonModels/ag-20230606_103643/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20230606_103643/predictor.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.timeseries.predictor.TimeSeriesPredictor at 0x7f1818f162e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data=train_data,\n",
    "              hyperparameters={\n",
    "            \"DeepAR\": {\"epochs\": 10}\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230606_104214/\"\n",
      "Loading predictor from path AutogluonModels/ag-20230606_103643/\n",
      "Loading: AutogluonModels/ag-20230606_103643/learner.pkl\n",
      "Loading: AutogluonModels/ag-20230606_103643/predictor.pkl\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Predictor is already fit! To fit additional models create a new `Predictor`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mysunk/PJT/automl_study/msun/deepar.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predictor \u001b[39m=\u001b[39m TimeSeriesPredictor(known_covariates_names\u001b[39m=\u001b[39mknown_covariates_cols, target\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msales\u001b[39m\u001b[39m\"\u001b[39m, prediction_length\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, validation_splitter\u001b[39m=\u001b[39msplitter,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                 eval_metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRMSE\u001b[39m\u001b[39m\"\u001b[39m, verbosity \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m predictor_saved \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mload(path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAutogluonModels/ag-20230606_103643/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m predictor_saved\u001b[39m.\u001b[39;49mfit(train_data\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m               hyperparameters\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mDeepAR\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m10\u001b[39;49m}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224131303033227d/home/mysunk/PJT/automl_study/msun/deepar.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             })\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/autogluon/core/utils/decorators.py:30\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/autogluon/timeseries/predictor.py:427\u001b[0m, in \u001b[0;36mTimeSeriesPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, hyperparameter_tune_kwargs, enable_ensemble, random_seed, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_learner\u001b[39m.\u001b[39mis_fit:\n\u001b[0;32m--> 427\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPredictor is already fit! To fit additional models create a new `Predictor`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m hyperparameters \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     hyperparameters \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Predictor is already fit! To fit additional models create a new `Predictor`."
     ]
    }
   ],
   "source": [
    "predictor = TimeSeriesPredictor(known_covariates_names=known_covariates_cols, target=\"sales\", prediction_length=16, validation_splitter=splitter,\n",
    "                                eval_metric=\"RMSE\", verbosity = 4)\n",
    "predictor_saved = predictor.load(path = \"AutogluonModels/ag-20230606_103643/\")\n",
    "predictor_saved.fit(train_data=train_data,\n",
    "              hyperparameters={\n",
    "            \"DeepAR\": {\"epochs\": 10}\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
